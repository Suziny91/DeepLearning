{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Iris Data Loader and DataFrame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Code !!\n",
    "iris = datasets.load_iris()\n",
    "iris\n",
    "\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X,y data Generator...Feature and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 속성과 라벨을 X, y에 할당\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Test 데이타를 8:2로 비율로 섞고, random_state=42로 지정\n",
    "    X_train, X_test, y_train, y_test 로 각각 할당된 값들을 torch 타입으로 변환 \n",
    "    torch.FloatTensor(), torch.LongTensor 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, \n",
    "                                                    iris.target, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=iris.target)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 56\n",
    "num_classes = 3\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    # 모델의 Forward Path를 정의\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Excution , loss, optimizer, backward ..\n",
    "    Forward Propagation and Baward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1/100], Loss : 1.1139\n",
      "Epoch : [2/100], Loss : 1.0216\n",
      "Epoch : [3/100], Loss : 0.9398\n",
      "Epoch : [4/100], Loss : 0.8616\n",
      "Epoch : [5/100], Loss : 0.7923\n",
      "Epoch : [6/100], Loss : 0.7295\n",
      "Epoch : [7/100], Loss : 0.6762\n",
      "Epoch : [8/100], Loss : 0.6268\n",
      "Epoch : [9/100], Loss : 0.5793\n",
      "Epoch : [10/100], Loss : 0.5382\n",
      "Epoch : [11/100], Loss : 0.5043\n",
      "Epoch : [12/100], Loss : 0.4737\n",
      "Epoch : [13/100], Loss : 0.4454\n",
      "Epoch : [14/100], Loss : 0.4199\n",
      "Epoch : [15/100], Loss : 0.3961\n",
      "Epoch : [16/100], Loss : 0.3741\n",
      "Epoch : [17/100], Loss : 0.3548\n",
      "Epoch : [18/100], Loss : 0.3370\n",
      "Epoch : [19/100], Loss : 0.3198\n",
      "Epoch : [20/100], Loss : 0.3038\n",
      "Epoch : [21/100], Loss : 0.2884\n",
      "Epoch : [22/100], Loss : 0.2739\n",
      "Epoch : [23/100], Loss : 0.2609\n",
      "Epoch : [24/100], Loss : 0.2486\n",
      "Epoch : [25/100], Loss : 0.2369\n",
      "Epoch : [26/100], Loss : 0.2261\n",
      "Epoch : [27/100], Loss : 0.2158\n",
      "Epoch : [28/100], Loss : 0.2062\n",
      "Epoch : [29/100], Loss : 0.1972\n",
      "Epoch : [30/100], Loss : 0.1885\n",
      "Epoch : [31/100], Loss : 0.1805\n",
      "Epoch : [32/100], Loss : 0.1730\n",
      "Epoch : [33/100], Loss : 0.1660\n",
      "Epoch : [34/100], Loss : 0.1595\n",
      "Epoch : [35/100], Loss : 0.1534\n",
      "Epoch : [36/100], Loss : 0.1478\n",
      "Epoch : [37/100], Loss : 0.1426\n",
      "Epoch : [38/100], Loss : 0.1378\n",
      "Epoch : [39/100], Loss : 0.1333\n",
      "Epoch : [40/100], Loss : 0.1292\n",
      "Epoch : [41/100], Loss : 0.1253\n",
      "Epoch : [42/100], Loss : 0.1217\n",
      "Epoch : [43/100], Loss : 0.1184\n",
      "Epoch : [44/100], Loss : 0.1153\n",
      "Epoch : [45/100], Loss : 0.1124\n",
      "Epoch : [46/100], Loss : 0.1098\n",
      "Epoch : [47/100], Loss : 0.1073\n",
      "Epoch : [48/100], Loss : 0.1050\n",
      "Epoch : [49/100], Loss : 0.1029\n",
      "Epoch : [50/100], Loss : 0.1009\n",
      "Epoch : [51/100], Loss : 0.0991\n",
      "Epoch : [52/100], Loss : 0.0974\n",
      "Epoch : [53/100], Loss : 0.0957\n",
      "Epoch : [54/100], Loss : 0.0943\n",
      "Epoch : [55/100], Loss : 0.0928\n",
      "Epoch : [56/100], Loss : 0.0915\n",
      "Epoch : [57/100], Loss : 0.0902\n",
      "Epoch : [58/100], Loss : 0.0891\n",
      "Epoch : [59/100], Loss : 0.0880\n",
      "Epoch : [60/100], Loss : 0.0869\n",
      "Epoch : [61/100], Loss : 0.0859\n",
      "Epoch : [62/100], Loss : 0.0850\n",
      "Epoch : [63/100], Loss : 0.0841\n",
      "Epoch : [64/100], Loss : 0.0833\n",
      "Epoch : [65/100], Loss : 0.0825\n",
      "Epoch : [66/100], Loss : 0.0817\n",
      "Epoch : [67/100], Loss : 0.0810\n",
      "Epoch : [68/100], Loss : 0.0803\n",
      "Epoch : [69/100], Loss : 0.0797\n",
      "Epoch : [70/100], Loss : 0.0790\n",
      "Epoch : [71/100], Loss : 0.0784\n",
      "Epoch : [72/100], Loss : 0.0778\n",
      "Epoch : [73/100], Loss : 0.0773\n",
      "Epoch : [74/100], Loss : 0.0767\n",
      "Epoch : [75/100], Loss : 0.0762\n",
      "Epoch : [76/100], Loss : 0.0757\n",
      "Epoch : [77/100], Loss : 0.0753\n",
      "Epoch : [78/100], Loss : 0.0748\n",
      "Epoch : [79/100], Loss : 0.0744\n",
      "Epoch : [80/100], Loss : 0.0739\n",
      "Epoch : [81/100], Loss : 0.0735\n",
      "Epoch : [82/100], Loss : 0.0731\n",
      "Epoch : [83/100], Loss : 0.0727\n",
      "Epoch : [84/100], Loss : 0.0723\n",
      "Epoch : [85/100], Loss : 0.0720\n",
      "Epoch : [86/100], Loss : 0.0716\n",
      "Epoch : [87/100], Loss : 0.0713\n",
      "Epoch : [88/100], Loss : 0.0709\n",
      "Epoch : [89/100], Loss : 0.0706\n",
      "Epoch : [90/100], Loss : 0.0703\n",
      "Epoch : [91/100], Loss : 0.0700\n",
      "Epoch : [92/100], Loss : 0.0696\n",
      "Epoch : [93/100], Loss : 0.0693\n",
      "Epoch : [94/100], Loss : 0.0691\n",
      "Epoch : [95/100], Loss : 0.0688\n",
      "Epoch : [96/100], Loss : 0.0685\n",
      "Epoch : [97/100], Loss : 0.0682\n",
      "Epoch : [98/100], Loss : 0.0680\n",
      "Epoch : [99/100], Loss : 0.0677\n",
      "Epoch : [100/100], Loss : 0.0675\n"
     ]
    }
   ],
   "source": [
    "# 위에서 정의한 클래스를 인스턴스화 시킴\n",
    "# model = NeuralNet(input_size, hidden_size, num_classes).to(device) # to(device) : 이 모델을 gpu 서버에서 돌린다는 뜻\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "# loss, optimizer를 선정의\n",
    "loss_function = nn.CrossEntropyLoss() # Loss 기능 안에 Softmax 함수 기능 포함되어져 있다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs): # 5번\n",
    "    # Forward Pass\n",
    "    pred = model(X_train)\n",
    "    loss = loss_function(pred, y_train)\n",
    "    \n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch : [{epoch + 1}/{num_epochs}], Loss : {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  학습을 마친 최종적인 모델의 값을 저장. model.ckpt 파일로 저장합니다.\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch(학습)에 따른 Loss감소를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs), loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 통해서 Loss를 감소시켰다면 이제는Test를 해봅니다.\n",
    "    테스트 할때는 학습의 의미가 없기때문에 Gradient Descent를 사용하지 않도록 합니다.\n",
    "    그 결과로 컴퓨터 Performance를 높이는 결과를 가져옵니다.\n",
    "    이때 우리가 테스트하는 데이타는 이미지가 아니고 단순 숫자 값으로 입력된다는 점을 잘 고려해야합니다.\n",
    "    출력된 값 중에서 가장 높은 값의 인덱스가 바로 target의 라벨이 됩니다.\n",
    "    \n",
    "    예측한 값과 정답을 일일이 비교해서 출력하고\n",
    "    총 30개의 Test 데이타 중에서 정확하게 맞춘 갯수를 최종적으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Network on the Test Images : 100.0%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # 미분 안하겠다...실제로 학습할 필요가 없을 때 이 구문을 반드시 작성\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in zip(X_test, y_test):\n",
    "        outputs = model(features)\n",
    "        \n",
    "        total += 1\n",
    "        correct += (torch.argmax(outputs) == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy of the Network on the Test Images : {100*correct/total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
