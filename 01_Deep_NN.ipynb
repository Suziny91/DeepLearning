{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 프레임 워크 불러오는 부분\n",
    "import torch \n",
    "import torchvision # image processing에 특화된 라이브러리\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms # Data Augmentation에 특화된 라이브러리\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(2,3) #2행 3열의 텐서를 하나 생성\n",
    "X.shape\n",
    "X.size()\n",
    "\n",
    "X = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tensor() 인자값으로 \n",
    "data : 값 지정, 초기화\n",
    "dtype\n",
    "requires_grad에 대해서 미분 적용할지 여부\n",
    "'''\n",
    "x = torch.tensor(data=[2.0,3.0], requires_grad=True)\n",
    "x\n",
    "\n",
    "y = x**2\n",
    "y\n",
    "\n",
    "pred = 2*y+3 # 11,21\n",
    "\n",
    "target = torch.tensor([3.0,4.0]) #3, 4\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n위에 나온 결과를 바탕으로 \\n경사하강법을 이용해서\\n미분을 사용해서 Loss에 대한 책임을 묻겠다.\\n기울기가 계산이 될 것이다.(미분 계산)\\n--> BackPropagation을 적용하려면 backward() 함수를 호출\\nLoss를 줄여나가 보겠다.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "위에 나온 결과를 바탕으로 \n",
    "경사하강법을 이용해서\n",
    "미분을 사용해서 Loss에 대한 책임을 묻겠다.\n",
    "기울기가 계산이 될 것이다.(미분 계산)\n",
    "--> BackPropagation을 적용하려면 backward() 함수를 호출\n",
    "Loss를 줄여나가 보겠다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum(torch.abs(pred - target))\n",
    "print('Loss',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 12.]) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f0672e5ef792>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(x.grad, pred.grad)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad, pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tens....Using NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4353,  0.6515,  0.1465],\n",
      "        [-0.1266, -1.3723, -0.7454],\n",
      "        [-0.5641, -0.2480, -0.3307],\n",
      "        [ 0.4668,  0.2616,  0.1623],\n",
      "        [-0.6891, -0.0710,  0.1789],\n",
      "        [-2.1267,  0.6610,  0.2195],\n",
      "        [-1.2965,  1.9744,  1.2464],\n",
      "        [-0.6071, -0.0032,  0.1651],\n",
      "        [-0.4434,  0.0761, -1.7083],\n",
      "        [-0.9013, -0.0233, -0.9140]])\n",
      "tensor([[-0.4619, -0.1061],\n",
      "        [ 1.5990,  0.3641],\n",
      "        [-1.1861,  0.0230],\n",
      "        [-1.2324,  0.9560],\n",
      "        [-0.6554, -0.1082],\n",
      "        [-0.1205, -1.1527],\n",
      "        [-0.2543, -0.4357],\n",
      "        [-1.7944,  0.9793],\n",
      "        [-0.1302,  0.0150],\n",
      "        [ 1.1660, -1.6269]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,3)\n",
    "y = torch.randn(10,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-0.3275,  0.5110, -0.1635],\n",
      "        [ 0.2884,  0.0657, -0.1736]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([-0.3761, -0.3629], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3,2)\n",
    "print('w', linear.weight)\n",
    "print('b', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001BED5DF26D0>\n",
      "loss before step BackPropagation 1.0043259859085083\n"
     ]
    }
   ],
   "source": [
    "# 객체출력, parameters() 함수는 모델안의 학습의 주체인 w, b를 내포하고 있는 객체\n",
    "# parameters() 함수는 w, b를 해킹한 함수이다.\n",
    "print(linear.parameters())\n",
    "\n",
    "# Loss function을 미리 선정의 해두었다.\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# optimizer(하산하는 방법)를 미리 정의\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# 위에서 만든 모델에서 값을 입력..결과로 예측값이 나온다.\n",
    "pred = linear(x)\n",
    "\n",
    "# 정답과 예측값을 이용해서 위에서 선정의한 Loss를 계산..L(W)\n",
    "loss = loss_function(pred, y)\n",
    "print('loss before step BackPropagation',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor([[-0.6124,  1.2488,  0.8481],\n",
      "        [ 0.0388,  0.0180, -0.2004]])\n",
      "dL/db tensor([ 0.4869, -0.7613])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loss값이 나왔다는 것은\n",
    "모델의 예측값에 대한 잘잘못을 정량화 했다는 것이다.\n",
    "\n",
    "이 값을 바탕으로 BackPropagation을 하면 w, b에 대한 미분값\n",
    "즉 얼마만큼 보정해야 하는지의 값이 나온다.\n",
    "그 값을 이용해서 다시 하강(기울기 수정)하기 때문에\n",
    "2번째에는 Loss가 줄어들어야 한다.\n",
    "'''\n",
    "loss.backward() # loss값에 대한 w의 책임을 묻는다.....미분을 적용\n",
    "print('dL/dw', linear.weight.grad)\n",
    "print('dL/db', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after BackPropagation :  0.9782593846321106\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() #위에서 수정된 값으로 하강을 진행한다...학습을 한다.\n",
    "\n",
    "#반복 효과\n",
    "pred = linear(x)\n",
    "loss = loss_function(pred,y)\n",
    "\n",
    "print('loss after BackPropagation : ', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
